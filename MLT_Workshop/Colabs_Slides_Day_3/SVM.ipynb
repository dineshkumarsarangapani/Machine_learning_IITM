{"cells":[{"cell_type":"markdown","metadata":{"id":"Kwt8A5yBXwAW"},"source":["# Support Vector Machines\n","\n","We wil implement both hard-margin SVMs and soft-margin SVMs from scratch on a toy dataset. Apart from `NumPy`, we would need to take the help of `SciPy` for solving the quadratic programming problem."]},{"cell_type":"markdown","metadata":{"id":"49WrewU6X1aR"},"source":["## Hard-Margin SVM"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"1bIDOxRZWRP9","executionInfo":{"status":"ok","timestamp":1671345941287,"user_tz":-330,"elapsed":39,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"CO6QjhLnWYX3","executionInfo":{"status":"ok","timestamp":1671345941288,"user_tz":-330,"elapsed":31,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":["#### DATA: DO NOT EDIT THIS CELL ####\n","X = np.array([[1, -3], [1, 0], [4, 1], [3, 7], [0, -2],\n","             [-1, -6], [2, 5], [1, 2], [0, -1], [-1, -4],\n","             [0, 7], [1, 5], [-4, 4], [2, 9], [-2, 2],\n","             [-2, 0], [-3, -2], [-2, -4], [3, 10], [-3, -8]]).T\n","y = np.array([1, 1, 1, 1, 1, \n","             1, 1, 1, 1, 1,\n","             -1, -1, -1, -1, -1, \n","             -1, -1, -1, -1, -1])"]},{"cell_type":"markdown","metadata":{"id":"nhWvMF3QYQdx"},"source":["### Understand the data\n","\n","$\\mathbf{X}$ is a data-matrix of shape $(d, n)$. $\\mathbf{y}$ is a vector of labels of size $(n, )$. Specifically, look at the shapes of the arrays involved."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"EOO0y6UaapAa","executionInfo":{"status":"ok","timestamp":1671345941289,"user_tz":-330,"elapsed":29,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"aI0L-xJ4aj-d"},"source":["### Visualize the dataset\n","\n","Visualize the dataset given to you using a scatter plot. Colour points which belong to class $+1$ $\\color{green}{\\text{green}}$ and those that belong to $-1$ $\\color{red}{\\text{red}}$. Inspect the data visually and determine its linear separability."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"kbjQsLMSXuDc","executionInfo":{"status":"ok","timestamp":1671345941289,"user_tz":-330,"elapsed":25,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":[]},{"cell_type":"markdown","source":["### Linear Separability\n","\n","Is there another way to determine linear separability?"],"metadata":{"id":"V4fH-yQH8IYX"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"w-Ifz3QH56DI","executionInfo":{"status":"ok","timestamp":1671345941290,"user_tz":-330,"elapsed":23,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"uDOb9EmzZwWG"},"source":["### Computing the Dual Objective\n","\n","We shall follow a step-by-step approach to computing the dual objective function."]},{"cell_type":"markdown","metadata":{"id":"jMdpPLXPo3Fi"},"source":["#### Step-1\n","\n","Compute the object $\\mathbf{Y}$ that appears in the dual problem."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"EzgoThG4aPej","executionInfo":{"status":"ok","timestamp":1671345941290,"user_tz":-330,"elapsed":20,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Zbbv04-qa_c3"},"source":["#### Step-2\n","\n","Let $\\boldsymbol{\\alpha}$ be the dual variable. The dual objective is of the form:\n","\n","$$\n","f(\\boldsymbol{\\alpha}) = \\boldsymbol{\\alpha}^T \\mathbf{1} - \\cfrac{1}{2} \\cdot \\boldsymbol{\\alpha}^T \\mathbf{Q} \\boldsymbol{\\alpha}\n","$$\n","\n","\n","\n","Compute the matrix $\\mathbf{Q}$ for this problem."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"p_AsPP_nb48r","executionInfo":{"status":"ok","timestamp":1671345941291,"user_tz":-330,"elapsed":20,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"O4QlhIsWyxYe"},"source":["#### Step-3\n","\n","Since `SciPy`'s optimization routines take the form of minimizing a function, we will recast $f$ as follows:\n","\n","$$\n","f(\\boldsymbol{\\alpha}) =  \\cfrac{1}{2} \\cdot \\boldsymbol{\\alpha}^T \\mathbf{Q} \\boldsymbol{\\alpha} - \\boldsymbol{\\alpha}^T \\mathbf{1}\n","$$\n","\n","We now have to solve :\n","\n","$$\n","\\min \\limits_{\\boldsymbol{\\alpha} \\geq 0} \\quad f(\\boldsymbol{\\alpha})\n","$$\n","\n","Note that $\\max$ changes to $\\min$ since we changed the sign of the objective function."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"TXGgoTZ4z63Z","executionInfo":{"status":"ok","timestamp":1671345941291,"user_tz":-330,"elapsed":18,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Plv87Si05V5G"},"source":["### Optimize\n","\n","Finally, we have most of the ingredients to solve the dual problem:\n","\n","$$\n","\\min \\limits_{\\boldsymbol{\\alpha} \\geq 0} \\quad \\cfrac{1}{2} \\cdot \\boldsymbol{\\alpha}^T \\mathbf{Q} \\boldsymbol{\\alpha} - \\boldsymbol{\\alpha}^T \\mathbf{1}\n","$$\n","\n","Find the optimal value, $\\boldsymbol{\\alpha^{*}}$."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"y_Wcmwy3grQ7","executionInfo":{"status":"ok","timestamp":1671345941291,"user_tz":-330,"elapsed":17,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"hrruRySrPPJL"},"source":["### Support vectors\n","\n","Find all the support vectors."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"d7JTzQniPZsg","executionInfo":{"status":"ok","timestamp":1671345941292,"user_tz":-330,"elapsed":18,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"VA4rcEt2NwCI"},"source":["### Optimal weight vector (Primal solution)\n","\n","Find the optimal weight vector $\\mathbf{w}^*$."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"WbIi8Snu6Qga","executionInfo":{"status":"ok","timestamp":1671345941292,"user_tz":-330,"elapsed":17,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"1pRJINcgRAWs"},"source":["### Decision Boundary\n","\n","Plot the decision boundary along with the supporting hyperplanes. Note where the support vectors lie in this plot."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2YiYiZyZbRUK","executionInfo":{"status":"ok","timestamp":1671345941293,"user_tz":-330,"elapsed":17,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"dsM9fbm_R-Vb"},"source":["## Soft-margin SVM\n","\n","We now turn to soft-margin SVMs. Adapt the hard-margin code that you have written for the soft-margin problem. The only change you have to make is to introduce an upper bound for $\\boldsymbol{\\alpha}$, which is the hyperparameter $C$.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"OXSn3CvMS-iT","executionInfo":{"status":"ok","timestamp":1671345941293,"user_tz":-330,"elapsed":17,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":["#### DATA: DO NOT EDIT THIS CELL ####\n","X = np.array([[1, -3], [1, 0], [4, 1], [3, 7], [0, -2],\n","             [-1, -6], [2, 5], [1, 2], [0, -1], [-1, -4],\n","             [0, 7], [1, 5], [-4, 4], [2, 9], [-2, 2],\n","             [-2, 0], [-3, -2], [-2, -4], [3, 10], [-3, -8],\n","             [0, 0], [2, 7]]).T\n","y = np.array([1, 1, 1, 1, 1, \n","             1, 1, 1, 1, 1,\n","             -1, -1, -1, -1, -1, \n","             -1, -1, -1, -1, -1,\n","              1, 1])"]},{"cell_type":"markdown","metadata":{"id":"1fngF_sW-jKD"},"source":["### Relationship between $C$ and margin\n","\n","Plot the decision boundary and the supporting hyperplane for the following values of $C$.\n","\n","(1) $C = 0.01$\n","\n","(2) $C = 0.1$\n","\n","(3) $C = 1$\n","\n","(4) $C = 10$\n","\n","Plot all of them in a $2 \\times 2$ subplot. Study the tradeoff between the following quantities:\n","\n","(1) Width of the margin.\n","\n","(2) Number of points that lie within the margin or on the wrong side. This is often called **margin violation**.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"vVKn4mZfTzJu","executionInfo":{"status":"ok","timestamp":1671345941293,"user_tz":-330,"elapsed":16,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"LAHR_IChAq4f"},"source":["### Support vectors\n","\n","For $C = 10$, study the number of support vectors that the model has."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"HAnccH_-CRqA","executionInfo":{"status":"ok","timestamp":1671345941294,"user_tz":-330,"elapsed":16,"user":{"displayName":"Karthik POD","userId":"18250337670386755696"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1BL6T_E7BVMU2koexzDznLjcn-ngWANdr","timestamp":1669191317195}],"authorship_tag":"ABX9TyO1Gu1gQZ8127ShRAOOFogS"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}